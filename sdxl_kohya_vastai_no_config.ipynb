{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d4773-223c-424a-a912-59ca687aa479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5194f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDXL Dreambooth training with Kohya_ss Scripts - No-config version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9888623-b6f4-4bd9-aa2d-e1234345991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. INSTALL DEPENDENCIES\n",
    "\n",
    "print(\"Installing dependencies - this might take a few minutes...\")\n",
    "%cd sd-scripts\n",
    "!apt-get update -y && apt-get install -y libgl1\n",
    "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118 # no_verify leave this to specify not checking this a verification stage\n",
    "!pip install xformers==0.0.20 bitsandbytes==0.35.0\n",
    "!pip install tensorboard==2.12.3 tensorflow==2.12.0 wheel\n",
    "!pip install protobuf==3.20.3\n",
    "!pip install tensorrt\n",
    "# !pip install --upgrade -r requirements.txt\n",
    "!pip install gdown\n",
    "!pip install accelerate==0.19.0\n",
    "!pip install transformers==4.30.2\n",
    "!pip install diffusers[torch]==0.18.2\n",
    "!pip install ftfy==6.1.1\n",
    "!pip install albumentations==1.3.0\n",
    "!pip install opencv-python==4.7.0.68\n",
    "!pip install einops==0.6.0\n",
    "!pip install pytorch-lightning==1.9.0\n",
    "!pip install bitsandbytes==0.35.0\n",
    "!pip install tensorboard==2.12.0\n",
    "!pip install safetensors==0.3.1\n",
    "!pip install # gradio==3.16.2\n",
    "!pip install altair==4.2.2\n",
    "!pip install easygui==0.98.3\n",
    "!pip install toml==0.10.2\n",
    "!pip install voluptuous==0.13.1\n",
    "!pip install huggingface-hub==0.15.1\n",
    "!pip install invisible-watermark==0.2.0 \n",
    "!pip install open-clip-torch==2.20.0\n",
    "!pip install protobuf==3.20.3\n",
    "!pip install -e .\n",
    "print(\"Finished installing dependencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "197e5f03-f37d-4a75-bd3c-b4ffd5b62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. VARIABLES\n",
    "# The following variables can be changed according to your project preferences\n",
    "token_word = \"ohwx\"\n",
    "class_word = \"person\" \n",
    "training_repeats = 40 \n",
    "training_root_dir = \"training_images\" \n",
    "regularization_root_dir = \"reg_images\"\n",
    "\n",
    "project_name = \"myProject\"\n",
    "output_dir = \"trained_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b52084e-b88e-462e-8b62-d814870b87bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7f292-afe1-4cdd-b639-edb2d14d6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CREATE DATASET FOLDERS\n",
    "\n",
    "# ================================================================\n",
    "# your training and reg image subfolders will be named according to what is set in previous cell: \n",
    "# so if you stuck with the defaults, they would be for example :\n",
    "# \"training_images/40_ohwx person\"\n",
    "# \"reg_images/1_person\"\n",
    "training_dir = f'{training_root_dir}/{training_repeats}_{token_word} {class_word}'\n",
    "reg_dir = f'{regularization_root_dir}/1_{class_word}'\n",
    "\n",
    "# back to parent folder\n",
    "%cd /workspace/sd-scripts\n",
    "\n",
    "import os\n",
    "if os.path.exists(training_dir) == False:\n",
    "  os.makedirs(training_dir)\n",
    "  print(f'{training_dir} Created.')\n",
    "else:\n",
    "  print(f'{training_dir} already exists.')\n",
    "\n",
    "if os.path.exists(reg_dir) == False:\n",
    "  os.makedirs(reg_dir)\n",
    "  print(f'{reg_dir} Created.')\n",
    "else:\n",
    "  print(f'{reg_dir} already exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02061da7-5923-4b79-ac3b-daa493d20337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download and unzip images to appropriate folders\n",
    "\n",
    "import shutil\n",
    "\n",
    "# download training images from google drive, rename to train.zip\n",
    "!gdown 1BIixbqMYW5-eOvTFC_RaZyWmlnBzey-0 -O train.zip\n",
    "\n",
    "# move train.zip to training images sub folder\n",
    "shutil.move('train.zip', f'{training_dir}')\n",
    "\n",
    "# extract zip contents to current folder and delete zip\n",
    "import zipfile\n",
    "%cd $training_dir\n",
    "with zipfile.ZipFile('train.zip', 'r') as train_ref:\n",
    "    train_ref.extractall()\n",
    "\n",
    "# delete zip\n",
    "!rm train.zip\n",
    "\n",
    "# back to parent folder\n",
    "%cd /workspace/sd-scripts\n",
    "\n",
    "# download the reg images from google drive, rename to regs.zip\n",
    "!gdown 1CIqOhLBfzrl5OkvGgbp2XK_njdFh5bMw -O regs.zip\n",
    "# move regs.zip to reg_images folder\n",
    "shutil.move('regs.zip', f'{reg_dir}')\n",
    "\n",
    "# extract zip contents to current folder and delete zip\n",
    "import zipfile\n",
    "%cd $reg_dir\n",
    "with zipfile.ZipFile('regs.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "# delete zip\n",
    "!rm regs.zip\n",
    "\n",
    "# back to parent folder\n",
    "%cd /workspace/sd-scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446f2c4-984a-48db-be94-3d0bfba0f57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c43b30d-afd9-450f-a7fb-e897407c6706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-04 14:12:38.722581: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-04 14:12:39.283185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    }
   ],
   "source": [
    "# 5. Set accelerate config with defaults\n",
    "!accelerate config default --mixed_precision \"bf16\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3acb2-df65-4515-b00e-4d002be7f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-04 14:14:37.741881: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-04 14:14:38.300904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-04 14:14:40.953691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "noise_offset is set to 0.0357 / noise_offsetが0.0357に設定されました\n",
      "prepare tokenizers\n",
      "Using DreamBooth method.\n",
      "prepare images.\n",
      "found directory training_images/40_ohwx person contains 14 image files\n",
      "No caption file found for 14 images. Training will continue without captions for these images. If class token exists, it will be used. / 14枚の画像にキャプションファイルが見つかりませんでした。これらの画像についてはキャプションなしで学習を続行します。class tokenが存在する場合はそれを使います。\n",
      "training_images/40_ohwx person/jp00.png\n",
      "training_images/40_ohwx person/jp01.png\n",
      "training_images/40_ohwx person/jp02.png\n",
      "training_images/40_ohwx person/jp03.png\n",
      "training_images/40_ohwx person/jp04.png\n",
      "training_images/40_ohwx person/jp05.png... and 9 more\n",
      "found directory reg_images/1_person contains 497 image files\n",
      "No caption file found for 497 images. Training will continue without captions for these images. If class token exists, it will be used. / 497枚の画像にキャプションファイルが見つかりませんでした。これらの画像についてはキャプションなしで学習を続行します。class tokenが存在する場合はそれを使います。\n",
      "reg_images/1_person/img_00001.png\n",
      "reg_images/1_person/img_00002.png\n",
      "reg_images/1_person/img_00003.png\n",
      "reg_images/1_person/img_00004.png\n",
      "reg_images/1_person/img_00005.png\n",
      "reg_images/1_person/img_00006.png... and 492 more\n",
      "560 train images with repeating.\n",
      "497 reg images.\n",
      "[Dataset 0]\n",
      "  batch_size: 2\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"training_images/40_ohwx person\"\n",
      "    image_count: 14\n",
      "    num_repeats: 40\n",
      "    shuffle_caption: False\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: ohwx person\n",
      "    caption_extension: .caption\n",
      "\n",
      "  [Subset 1 of Dataset 0]\n",
      "    image_dir: \"reg_images/1_person\"\n",
      "    image_count: 497\n",
      "    num_repeats: 1\n",
      "    shuffle_caption: False\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: True\n",
      "    class_tokens: person\n",
      "    caption_extension: .caption\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|██████████████████████████████████████| 511/511 [00:00<00:00, 16152.00it/s]\n",
      "prepare dataset\n",
      "prepare accelerator\n",
      "loading model for process 0/1\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=None\n",
      "The config attributes {'force_upcast': True} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "U-Net converted to original U-Net\n",
      "Disable Diffusers' xformers\n",
      "Enable xformers for U-Net\n",
      "[Dataset 0]\n",
      "caching latents.\n",
      "checking cache validity...\n",
      "100%|████████████████████████████████████| 511/511 [00:00<00:00, 1488395.38it/s]\n",
      "caching latents...\n",
      "100%|█████████████████████████████████████████| 511/511 [01:25<00:00,  5.98it/s]\n",
      "enable text encoder training\n",
      "number of models: 3\n",
      "number of trainable parameters: 3385184004\n",
      "prepare optimizer, data loader etc.\n",
      "use Adafactor optimizer | {'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\n",
      "enable full bf16 training.\n",
      "running training / 学習開始\n",
      "  num examples / サンプル数: 560\n",
      "  num batches per epoch / 1epochのバッチ数: 560\n",
      "  num epochs / epoch数: 4\n",
      "  batch size per device / バッチサイズ: 2\n",
      "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
      "  total optimization steps / 学習ステップ数: 2000\n",
      "steps:   0%|                                           | 0/2000 [00:00<?, ?it/s]\n",
      "epoch 1/4\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "steps:  25%|█████▎               | 500/2000 [15:19<45:58,  1.84s/it, loss=0.112]\n",
      "saving checkpoint: trained_models/myProject-step00000500.safetensors\n",
      "steps:  28%|█████▉               | 560/2000 [17:24<44:45,  1.86s/it, loss=0.111]\n",
      "epoch 2/4\n",
      "steps:  50%|██████████          | 1000/2000 [30:56<30:56,  1.86s/it, loss=0.104]\n",
      "saving checkpoint: trained_models/myProject-step00001000.safetensors\n",
      "steps:  56%|███████████▏        | 1120/2000 [34:46<27:19,  1.86s/it, loss=0.105]\n",
      "epoch 3/4\n",
      "steps:  69%|█████████████▋      | 1371/2000 [42:29<19:29,  1.86s/it, loss=0.105]"
     ]
    }
   ],
   "source": [
    "# 6. START TRAINING\n",
    "\n",
    "!accelerate launch --num_cpu_threads_per_process=2 \"./sdxl_train.py\" \\\n",
    "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
    "  --train_data_dir=\"{training_root_dir}\" \\\n",
    "  --reg_data_dir=\"{regularization_root_dir}\" \\\n",
    "  --output_dir=\"{output_dir}\" \\\n",
    "  --output_name=\"{project_name}\" \\\n",
    "  --save_model_as=\"safetensors\" \\\n",
    "  --train_batch_size=2 \\\n",
    "  --max_train_steps=2000 \\\n",
    "  --save_every_n_steps=500 \\\n",
    "  --optimizer_type=\"adafactor\" \\\n",
    "  --optimizer_args scale_parameter=False relative_step=False warmup_init=False \\\n",
    "  --xformers \\\n",
    "  --cache_latents \\\n",
    "  --lr_scheduler=\"constant_with_warmup\" \\\n",
    "  --lr_warmup_steps=100 \\\n",
    "  --learning_rate=1e-5 \\\n",
    "  --max_grad_norm=0.0 \\\n",
    "  --train_text_encoder \\\n",
    "  --resolution=\"1024,1024\" \\\n",
    "  --save_precision=\"float\" \\\n",
    "  --save_n_epoch_ratio=1 \\\n",
    "  --max_data_loader_n_workers=1 \\\n",
    "  --persistent_data_loader_workers \\\n",
    "  --mixed_precision=\"bf16\" \\\n",
    "  --full_bf16 \\\n",
    "  --logging_dir=\"logs\" \\\n",
    "  --log_prefix=\"last\" \\\n",
    "  --gradient_checkpointing\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44072bf-090f-4adf-bece-8f9d3b5db145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
